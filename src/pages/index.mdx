---
layout: ../layouts/Layout.astro
title: "Morphics matters on Learning"
authors:
  - name: Kang Minkyu
  - name: Kim Kihyeon
  - name: Pyo Sanghun
favicon: favicon.svg
description: "Project page for Morphics matters on Learning"
---

import Authors from "../components/Authors.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import TwoColumns from "../components/TwoColumns.astro";
import LaTeX from "../components/LaTeX.astro";
import Table from "../components/Table.astro";
import { Comparison } from "../components/Comparison.tsx";
import Video from "../components/Video.astro";
import Links from "../components/Links.astro";
import Figure from "../components/Figure.astro";
import FrontRocket from "../assets/front-rocket.jpeg"; // PNG 대신 JPEG로 변경
import TopViewRocket from "../assets/top-view-rocket.jpeg"; // PNG 대신 JPEG로 변경
import TieAdvanced from "../assets/tie-advanced.jpeg";
import NinjaBoy from "../assets/ninja_boy.gif"; // GIF 파일 임포트

<Figure items={[
  {
    src: NinjaBoy.src,
    alt: "Ninja Boy GIF",
  },
]} />

<HighlightedSection>

## Abstract

During the learning process of reinforcement learning and imitation reinforcement learning,
the agent learns to perform a task by interacting with the environment. However, the agent may not always learn the optimal policy.
In this case, the agent may learn a suboptimal policy that is not as good as the optimal policy. We are going to introduce about
the effect of morphics on learning in reinforcement learning and imitation reinforcement learning for space travle situation.
And how morphics can help the agent to learn the optimal policy faster and better.

</HighlightedSection>

## Motivation

The morphological characteristics of an agent significantly influence its learning capabilities in reinforcement learning environments. By carefully designing the agent\'s morphology, we can enhance its ability to explore the state space and learn optimal policies more efficiently. This is particularly crucial in complex environments like space travel scenarios, where optimal policy learning can be challenging due to the high-dimensional state and action spaces.

## Algorithm

We utilize two primary algorithms for our reinforcement learning framework: Proximal Policy Optimization (PPO) and Generative Adversarial Imitation Learning (GAIL).

### Proximal Policy Optimization (PPO)

PPO is a policy gradient method that prevents large policy updates through a clipped objective function. The key equation for PPO is:

<LaTeX formula={`L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} \\hat{A}_t, \\text{clip} \\left( \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}, 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}_t \\right) \\right]`} />

where <LaTeX formula={`\\pi_\\theta`} inline /> is the policy, <LaTeX formula={`\\hat{A}_t`} inline /> is the advantage estimate, and <LaTeX formula={`\\epsilon`} inline /> is the clipping parameter.

### Generative Adversarial Imitation Learning (GAIL)

GAIL combines imitation learning with adversarial training, using a discriminator to distinguish between expert and agent behaviors. The objective function is:

<LaTeX formula={`\\min_{\\pi} \\max_{D} V(D, \\pi) = \\mathbb{E}_{\\pi} [\\log D(s,a)] + \\mathbb{E}_{\\pi_E} [\\log (1 - D(s,a))]`} />

where <LaTeX formula={`\\pi`} inline /> is the agent policy, <LaTeX formula={`\\pi_E`} inline /> is the expert policy, and <LaTeX formula={`D`} inline /> is the discriminator.

## Tie Advanced: A Star Wars Inspired Morphology

<figure class="flex flex-col items-center my-4">
  <img src={TieAdvanced.src} alt="Tie Advanced" class="max-w-md h-auto rounded-lg shadow-md" />
  <figcaption class="text-center mt-2 text-sm text-gray-600">This is the Tie Advanced image.</figcaption>
</figure>

One of the special morphologies we explore is the "Tie Advanced," inspired by the iconic Star Wars spaceship. We adapted this design into a learning rocket form. Below, you can see the "Front-Rocket" and "Top View Rocket" which represent the transformed morphology we used for our experiments.

<Figure items={[
  { src: FrontRocket.src, alt: "Front view of the rocket", caption: "Front view of our \"Tie Advanced\" inspired rocket." },
  { src: TopViewRocket.src, alt: "Top view of our \"Tie Advanced\" inspired rocket.", caption: "Top view of our \"Tie Advanced\" inspired rocket." }
]} />

## Appendix

### Detailed PPO Algorithm

The complete objective function for Proximal Policy Optimization (PPO) is designed to ensure stable and reliable policy updates. It combines three key components: the clipped surrogate objective, a value function loss, and an entropy bonus.

The final objective function is given by:
<LaTeX formula={`L_t^{PPO}(\\theta) = \\hat{\\mathbb{E}}_t [L_t^{CLIP}(\\theta) - c_1 L_t^{VF}(\\theta) + c_2 S[\\pi_\\theta](s_t)]`} />

Let's break down each part of this equation.

#### 1. Clipped Surrogate Objective (<LaTeX formula={`L^{CLIP}`} inline />)

This is the core of PPO's strategy to prevent destructively large policy updates.
<LaTeX formula={`L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right]`} />

*   **Derivation and Meaning:**
    *   <LaTeX formula={`r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}`} inline />: This is the probability ratio between the new policy (<LaTeX formula={`\\pi_\\theta`} inline />) and the old policy (<LaTeX formula={`\\pi_{\\theta_{old}}`} inline />). It measures how much the policy has changed. If <LaTeX formula={`r_t(\\theta) > 1`} inline />, the action <LaTeX formula={`a_t`} inline /> is more likely under the new policy.
    *   <LaTeX formula={`\\hat{A}_t`} inline />: This is the **Advantage Function**, which estimates how much better a given action <LaTeX formula={`a_t`} inline /> is compared to the average action at state <LaTeX formula={`s_t`} inline />. A positive advantage means the action was good.
    *   <LaTeX formula={`\\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t`} inline />: This is the "clipped" version. The `clip` function constrains the probability ratio <LaTeX formula={`r_t(\\theta)`} inline /> to stay within the range <LaTeX formula={`[1 - \\epsilon, 1 + \\epsilon]`} inline />, where <LaTeX formula={`\\epsilon`} inline /> is a small hyperparameter (e.g., 0.2).
    *   <LaTeX formula={`\\min(...)`} inline />: PPO takes the minimum of the normal objective and the clipped objective. This creates a pessimistic bound on the policy update, ensuring that each update step does not deviate too far from the previous policy, leading to more stable learning.

#### 2. Value Function Loss (<LaTeX formula={`L^{VF}`} inline />)

To calculate the advantage <LaTeX formula={`\\hat{A}_t`} inline />, we need a value function <LaTeX formula={`V(s_t)`} inline /> that estimates the expected return from a state.
<LaTeX formula={`L_t^{VF}(\\theta) = (V_\\theta(s_t) - V_t^{targ})^2`} />

*   **Derivation and Meaning:**
    *   <LaTeX formula={`V_\\theta(s_t)`} inline />: This is the value predicted by our value function network.
    *   <LaTeX formula={`V_t^{targ}`} inline />: This is the "target" value, usually calculated from the actual rewards received.
    *   **Implication:** This is a mean-squared error loss. By minimizing it, we train the value function to be an accurate predictor of future returns, which leads to a more accurate advantage estimate.

#### 3. Entropy Bonus (`S`)

To encourage exploration, an entropy bonus is added.
<LaTeX formula={`S[\\pi_\\theta](s_t)`} />

*   **Meaning and Implication:** Entropy measures the randomness of the policy. By adding entropy to the objective, we create an incentive for the policy to maintain a degree of randomness, helping the agent to continue exploring and discover better strategies.

#### 4. PPO Learning Process (Pseudocode)

<div style={{background: '#f7f7f7', border: '1px solid #ddd', padding: '15px', borderRadius: '5px', fontFamily: 'monospace', whiteSpace: 'pre-wrap', lineHeight: '1.5', fontSize: '14px'}}>
    <strong>Algorithm 1:</strong> Proximal Policy Optimization (PPO)
    <hr style={{margin: '10px 0'}} />
    1. Initialize policy network <LaTeX formula="\pi_\theta" inline /> and value network <LaTeX formula="V_\phi" inline /><br/>
    2. <strong>for</strong> iteration = 1, 2, ... <strong>do</strong><br/>
    3. &nbsp;&nbsp;&nbsp;&nbsp;Collect set of trajectories <LaTeX formula="\mathcal{D}_k = \{\tau_i\}" inline /> by running policy <LaTeX formula="\pi_k = \pi_{\theta_{\text{old}}}" inline /><br/>
    4. &nbsp;&nbsp;&nbsp;&nbsp;Compute rewards-to-go <LaTeX formula="\hat{R}_t" inline /><br/>
    5. &nbsp;&nbsp;&nbsp;&nbsp;Compute advantage estimates <LaTeX formula="\hat{A}_t" inline /> based on the current value function <LaTeX formula="V_{\phi_k}" inline /><br/>
    6. &nbsp;&nbsp;&nbsp;&nbsp;<strong>for</strong> epoch = 1, 2, ... K <strong>do</strong><br/>
    7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update policy by maximizing the PPO-Clip objective:<br/>
    <LaTeX formula={`\\theta_{k+1} \\leftarrow \\arg\\max_\\theta \\frac{1}{|\\mathcal{D}_k|T} \\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^{T} \\min\\left( \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_k}(a_t|s_t)} \\hat{A}_t, \\text{clip}\\left(\\frac{\pi_\\theta(a_t|s_t)/\\pi_{\\theta_k}(a_t|s_t)}, 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_t \\right)`} />
    8. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update value function by minimizing mean-squared error:<br/>
    <LaTeX formula={`\\phi_{k+1} \\leftarrow \\arg\\min_\\phi \\frac{1}{|\\mathcal{D}_k|T} \\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^{T} (V_\\phi(s_t) - \\hat{R}_t)^2`} />
    9. &nbsp;&nbsp;&nbsp;&nbsp;<strong>end for</strong><br/>
    10. <strong>end for</strong>
</div>

---

### Detailed GAIL Algorithm

Generative Adversarial Imitation Learning (GAIL) reframes imitation learning as an adversarial game, inspired by Generative Adversarial Networks (GANs).

The core objective function is a minimax game between the policy (the "Generator", <LaTeX formula={`\\pi`} inline />) and a "Discriminator" (<LaTeX formula={`D`} inline />):
<LaTeX formula={`\\min_{\\pi} \\max_{D} V(D, \\pi) = \\mathbb{E}_{\\pi} [\\log D(s,a)] + \\mathbb{E}_{\\pi_E} [\\log (1 - D(s,a))]`} />

Let's dissect this process.

#### 1. The Adversarial Game

*   **The Discriminator (`D`):**
    *   **Goal:** To distinguish between state-action pairs `(s, a)` from the agent's policy (<LaTeX formula={`\\pi`} inline />) and those from the expert's demonstrations (<LaTeX formula={`\\pi_E`} inline />).
    *   **Mechanism:** <LaTeX formula={`D(s, a)`} inline /> outputs the probability that the pair `(s, a)` came from the expert. It is trained to output <LaTeX formula={`D(s,a) \\to 1`} inline /> for expert data and <LaTeX formula={`D(s,a) \\to 0`} inline /> for agent data.

*   **The Policy (<LaTeX formula={`\\pi`} inline />, the Generator):**
    *   **Goal:** To "fool" the discriminator by generating trajectories that are so similar to the expert's that the discriminator outputs <LaTeX formula={`D(s,a) \\to 1`} inline />.

#### 2. GAIL Learning Process (Pseudocode)

<div style={{background: '#f7f7f7', border: '1px solid #ddd', padding: '15px', borderRadius: '5px', fontFamily: 'monospace', whiteSpace: 'pre-wrap', lineHeight: '1.5', fontSize: '14px'}}>
    <strong>Algorithm 2:</strong> Generative Adversarial Imitation Learning (GAIL)
    <hr style={{margin: '10px 0'}} />
    1. <strong>Input:</strong> Expert trajectories <LaTeX formula="\tau_E" inline /><br/>
    2. Initialize policy <LaTeX formula="\pi_\theta" inline /> and discriminator <LaTeX formula="D_\omega" inline /><br/>
    3. <strong>for</strong> iteration = 1, 2, ... <strong>do</strong><br/>
    4. &nbsp;&nbsp;&nbsp;&nbsp;Sample trajectories <LaTeX formula="\tau_\pi \sim \pi_\theta(a|s)" inline /><br/>
    5. &nbsp;&nbsp;&nbsp;&nbsp;Update discriminator <LaTeX formula="D_\omega" inline /> by ascending the gradient of:<br/>
    <LaTeX formula={`\\hat{\\mathbb{E}}_{\\tau_\\pi}[\\log(D_\\omega(s,a))] + \\hat{\\mathbb{E}}_{\\tau_E}[\\log(1 - D_\\omega(s,a))]`} />
    6. &nbsp;&nbsp;&nbsp;&nbsp;Define reward for the policy as <LaTeX formula="r(s,a) = -\log(1 - D_\omega(s,a))" inline /><br/>
    7. &nbsp;&nbsp;&nbsp;&nbsp;Update policy <LaTeX formula="\pi_\theta" inline /> using RL (e.g., PPO) with reward function <LaTeX formula="r(s,a)" inline /><br/>
    8. <strong>end for</strong>
</div>